CountVectorizer and TF-IDF:

import os
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report
from sklearn.model_selection import train_test_split

# Read the data from the CSV file and adding it in a dataframe
current_directory = os.getcwd()
processed_path = os.path.join(current_directory, 'data', 'train_processed+.csv')
df = pd.read_csv(processed_path)
df = df[df['comment_text_clean'].notna()]

# Creating and adding the 'any_toxic' label
df["any_toxic"] = df[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].any(axis=1).astype(int)

# Vectorizer settings of TF-IDF and CountVectorizer
print("Using TfidfVectorizer")
vectorizer = TfidfVectorizer(
    lowercase=False,    # already lowered
    stop_words=None,    # stop words already removed
    ngram_range=(1, 2), # using unigrams en bigrams
    max_features=5000   # limiting vocabulary size
)
# print("Using CountVectorizer")
# vectorizer = CountVectorizer(
#     lowercase=False,    # already lowered
#     stop_words=None,    # stop words already removed
#     ngram_range=(1, 1), # using bigrams
#     max_features=5000   # limiting vocabulary size
# )

# Splitting data in train (80%) and test (20%)
labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate', 'any_toxic']
X = df['comment_text_clean']
y = df[labels + ['any_toxic']]
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y['toxic']
)

# Vectorizing data
X_train_vec = vectorizer.fit_transform(X_train)
X_test_vec = vectorizer.transform(X_test)

# Training and evaluating each label
for label in labels:
    print(f"\n Training Random Forest for label: {label} (testset size: {y_test[label].shape[0]})")
    clf = RandomForestClassifier(n_estimators=100, class_weight='balanced', min_samples_leaf=2 , min_samples_split=20, random_state=42, n_jobs=-1)
    clf.fit(X_train_vec, y_train[label])
    y_pred = clf.predict(X_test_vec)
    print(classification_report(y_test[label], y_pred))










GloVe embeddings:

import os
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Load the dataset
df = pd.read_csv('/content/train_processed+.csv')
df = df[df['comment_text_clean'].notna()]

# Define target labels
labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']

# Tokenizer settings
MAX_NUM_WORDS = 20000
MAX_SEQUENCE_LENGTH = 200

# Tokenize the cleaned comments
tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)
tokenizer.fit_on_texts(df['comment_text_clean'])
sequences = tokenizer.texts_to_sequences(df['comment_text_clean'])

# Build word index and pad sequences
word_index = tokenizer.word_index
data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)
y = df[labels].values

# Load GloVe embeddings
embedding_dim = 100
glove_path = 'glove.twitter.27B.100d.txt'
embeddings_index = {}

with open(glove_path, encoding='utf8') as f:
    for line in f:
        values = line.split()
        word = values[0]
        coefs = np.asarray(values[1:], dtype='float32')
        embeddings_index[word] = coefs

# Build the embedding matrix
embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))
for word, i in word_index.items():
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        embedding_matrix[i] = embedding_vector

# Compute average GloVe embeddings per comment
def get_average_embeddings(sequences, embedding_matrix):
    avg_vectors = np.zeros((len(sequences), embedding_matrix.shape[1]))
    for i, seq in enumerate(sequences):
        valid_embeddings = [embedding_matrix[idx] for idx in seq if idx < embedding_matrix.shape[0] and np.any(embedding_matrix[idx])]
        if valid_embeddings:
            avg_vectors[i] = np.mean(valid_embeddings, axis=0)
    return avg_vectors

X_embed = get_average_embeddings(sequences, embedding_matrix)

# Split into train/test sets
X_train, X_test, y_train, y_test = train_test_split(
    X_embed, y, test_size=0.2, random_state=42, stratify=df['toxic']
)

# Train a Random Forest for each label
for i, label in enumerate(labels):
    print(f"\nTraining Random Forest for label: {label}")
    clf = RandomForestClassifier(
        n_estimators=200,
        class_weight='balanced',
        min_samples_leaf=2,
        min_samples_split=20,
        n_jobs=-1,
        random_state=42
    )
    clf.fit(X_train, y_train[:, i])
    y_pred = clf.predict(X_test)
    print(classification_report(y_test[:, i], y_pred))

# Train Random Forest for 'any_toxic' (if any label is 1)
y_train_any = (y_train.sum(axis=1) > 0).astype(int)
y_test_any = (y_test.sum(axis=1) > 0).astype(int)

print("\nTraining Random Forest for overall toxic vs non-toxic")
clf_any = RandomForestClassifier(
    n_estimators=200,
    class_weight='balanced',
    min_samples_leaf=2,
    min_samples_split=20,
    n_jobs=-1,
    random_state=42
)
clf_any.fit(X_train, y_train_any)
y_pred_any = clf_any.predict(X_test)

print("\nClassification report for label: any_toxic")
print(classification_report(y_test_any, y_pred_any))
