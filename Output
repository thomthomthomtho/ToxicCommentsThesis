"/Users/thomdeveling/Documents/Kunstmatige Intelligentie/Jaar 3/Thesis/.venv/bin/python" "/Users/thomdeveling/Documents/Kunstmatige Intelligentie/Jaar 3/Thesis/CountVectorizer(1,2).py"
(.venv) (base) thomdeveling@MacBook-Air-van-Thom-3 Thesis % "/Users/thomdeveling/Documents/Kunstmatige Intelligentie/Jaar 3/Thesis/.venv/bin/python" "/
Users/thomdeveling/Documents/Kunstmatige Intelligentie/Jaar 3/Thesis/CountVectorizer(1,2).py"
Hieronder het aantal NaN comments:
108
Vocab grootte: 20000
Matrix shape: (159463, 20000)

 Training SVM voor label: toxic (testset size: 31893)
              precision    recall  f1-score   support

           0       0.97      0.99      0.98     28834
           1       0.86      0.68      0.76      3059

    accuracy                           0.96     31893
   macro avg       0.92      0.84      0.87     31893
weighted avg       0.96      0.96      0.96     31893


 Now comes the next label

 Training SVM voor label: severe_toxic (testset size: 31893)
              precision    recall  f1-score   support

           0       0.99      1.00      1.00     31608
           1       0.52      0.28      0.37       285

    accuracy                           0.99     31893
   macro avg       0.76      0.64      0.68     31893
weighted avg       0.99      0.99      0.99     31893


 Now comes the next label

 Training SVM voor label: obscene (testset size: 31893)
              precision    recall  f1-score   support

           0       0.98      0.99      0.99     30199
           1       0.88      0.70      0.78      1694

    accuracy                           0.98     31893
   macro avg       0.93      0.85      0.88     31893
weighted avg       0.98      0.98      0.98     31893


 Now comes the next label

 Training SVM voor label: threat (testset size: 31893)
              precision    recall  f1-score   support

           0       1.00      1.00      1.00     31798
           1       0.52      0.18      0.27        95

    accuracy                           1.00     31893
   macro avg       0.76      0.59      0.63     31893
weighted avg       1.00      1.00      1.00     31893


 Now comes the next label

 Training SVM voor label: insult (testset size: 31893)
              precision    recall  f1-score   support

           0       0.98      0.99      0.98     30324
           1       0.78      0.56      0.65      1569

    accuracy                           0.97     31893
   macro avg       0.88      0.78      0.82     31893
weighted avg       0.97      0.97      0.97     31893


 Now comes the next label

 Training SVM voor label: identity_hate (testset size: 31893)
              precision    recall  f1-score   support

           0       0.99      1.00      1.00     31632
           1       0.59      0.25      0.35       261

    accuracy                           0.99     31893
   macro avg       0.79      0.62      0.67     31893
weighted avg       0.99      0.99      0.99     31893


WITH USE OF BALANCED:
Vocab size: 20000
Matrix shape: (159463, 20000)

 Training SVM for label: toxic (testset size: 31893)
              precision    recall  f1-score   support

           0       0.98      0.95      0.96     28834
           1       0.62      0.82      0.71      3059

    accuracy                           0.93     31893
   macro avg       0.80      0.89      0.84     31893
weighted avg       0.95      0.93      0.94     31893


 Now comes the next label

 Training SVM for label: severe_toxic (testset size: 31893)
              precision    recall  f1-score   support

           0       1.00      0.98      0.99     31608
           1       0.26      0.72      0.38       285

    accuracy                           0.98     31893
   macro avg       0.63      0.85      0.68     31893
weighted avg       0.99      0.98      0.98     31893


 Now comes the next label

 Training SVM for label: obscene (testset size: 31893)
              precision    recall  f1-score   support

           0       0.99      0.98      0.98     30199
           1       0.66      0.83      0.74      1694

    accuracy                           0.97     31893
   macro avg       0.83      0.90      0.86     31893
weighted avg       0.97      0.97      0.97     31893


 Now comes the next label

 Training SVM for label: threat (testset size: 31893)
              precision    recall  f1-score   support

           0       1.00      1.00      1.00     31798
           1       0.28      0.46      0.35        95

    accuracy                           0.99     31893
   macro avg       0.64      0.73      0.67     31893
weighted avg       1.00      0.99      1.00     31893


 Now comes the next label

 Training SVM for label: insult (testset size: 31893)
              precision    recall  f1-score   support

           0       0.99      0.96      0.98     30324
           1       0.52      0.80      0.63      1569

    accuracy                           0.95     31893
   macro avg       0.76      0.88      0.80     31893
weighted avg       0.97      0.95      0.96     31893


 Now comes the next label

 Training SVM for label: identity_hate (testset size: 31893)
              precision    recall  f1-score   support

           0       1.00      0.98      0.99     31632
           1       0.22      0.61      0.33       261

    accuracy                           0.98     31893
   macro avg       0.61      0.79      0.66     31893
weighted avg       0.99      0.98      0.98     31893


MEER VERANDERING !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
(.venv) (base) thomdeveling@MacBook-Air-van-Thom-3 Thesis % "/Users/thomdeveling/Documents/Kunstmatige Intelligentie/Jaar 3/Thesis/.venv/bin/python" "/
Users/thomdeveling/Documents/Kunstmatige Intelligentie/Jaar 3/Thesis/CountVectorizer(1,2).py"
Vocab size: 150000
Matrix shape: (159463, 150000)

 Training SVM for label: toxic (testset size: 31893)
              precision    recall  f1-score   support

           0       0.98      0.97      0.97     28834
           1       0.72      0.80      0.76      3059

    accuracy                           0.95     31893
   macro avg       0.85      0.88      0.87     31893
weighted avg       0.95      0.95      0.95     31893


 Training SVM for label: severe_toxic (testset size: 31893)
              precision    recall  f1-score   support

           0       1.00      0.99      0.99     31608
           1       0.32      0.60      0.41       285

    accuracy                           0.98     31893
   macro avg       0.66      0.80      0.70     31893
weighted avg       0.99      0.98      0.99     31893


 Training SVM for label: obscene (testset size: 31893)
              precision    recall  f1-score   support

           0       0.99      0.99      0.99     30199
           1       0.77      0.81      0.79      1694

    accuracy                           0.98     31893
   macro avg       0.88      0.90      0.89     31893
weighted avg       0.98      0.98      0.98     31893


 Training SVM for label: threat (testset size: 31893)
              precision    recall  f1-score   support

           0       1.00      1.00      1.00     31798
           1       0.37      0.36      0.36        95

    accuracy                           1.00     31893
   macro avg       0.68      0.68      0.68     31893
weighted avg       1.00      1.00      1.00     31893


 Training SVM for label: insult (testset size: 31893)
              precision    recall  f1-score   support

           0       0.99      0.98      0.98     30324
           1       0.64      0.75      0.69      1569

    accuracy                           0.97     31893
   macro avg       0.81      0.87      0.84     31893
weighted avg       0.97      0.97      0.97     31893


 Training SVM for label: identity_hate (testset size: 31893)
              precision    recall  f1-score   support

           0       1.00      0.99      0.99     31632
           1       0.36      0.54      0.44       261

    accuracy                           0.99     31893
   macro avg       0.68      0.77      0.71     31893
weighted avg       0.99      0.99      0.99     31893

(.venv) (base) thomdeveling@MacBook-Air-van-Thom-3 Thesis % "/Users/thomdeveling/Documents/Kunstmatige Intelligentie/Jaar 3/Thesis/.venv/bin/python" "/
Users/thomdeveling/Documents/Kunstmatige Intelligentie/Jaar 3/Thesis/CountVectorizer(1,2).py"
Vocab size: 2439948
Matrix shape: (159463, 2439948)

 Training SVM for label: toxic (testset size: 31893)
              precision    recall  f1-score   support

           0       0.98      0.98      0.98     28834
           1       0.79      0.78      0.78      3059

    accuracy                           0.96     31893
   macro avg       0.88      0.88      0.88     31893
weighted avg       0.96      0.96      0.96     31893


 Training SVM for label: severe_toxic (testset size: 31893)
              precision    recall  f1-score   support

           0       1.00      0.99      0.99     31608
           1       0.35      0.52      0.42       285

    accuracy                           0.99     31893
   macro avg       0.67      0.75      0.71     31893
weighted avg       0.99      0.99      0.99     31893


 Training SVM for label: obscene (testset size: 31893)
              precision    recall  f1-score   support

           0       0.99      0.99      0.99     30199
           1       0.81      0.82      0.82      1694

    accuracy                           0.98     31893
   macro avg       0.90      0.90      0.90     31893
weighted avg       0.98      0.98      0.98     31893


 Training SVM for label: threat (testset size: 31893)
              precision    recall  f1-score   support

           0       1.00      1.00      1.00     31798
           1       0.47      0.32      0.38        95

    accuracy                           1.00     31893
   macro avg       0.73      0.66      0.69     31893
weighted avg       1.00      1.00      1.00     31893


 Training SVM for label: insult (testset size: 31893)
              precision    recall  f1-score   support

           0       0.99      0.98      0.98     30324
           1       0.68      0.75      0.71      1569

    accuracy                           0.97     31893
   macro avg       0.83      0.87      0.85     31893
weighted avg       0.97      0.97      0.97     31893


 Training SVM for label: identity_hate (testset size: 31893)
              precision    recall  f1-score   support

           0       1.00      1.00      1.00     31632
           1       0.49      0.49      0.49       261

    accuracy                           0.99     31893
   macro avg       0.74      0.74      0.74     31893
weighted avg       0.99      0.99      0.99     31893



CROSS VALIDATION OUTPUTS:
Tfidf vectorizer (max features, max_iter=5000)
Label	         Fold 1	Fold 2	Fold 3	Fold 4	Fold 5	Mean
toxic	         0.78	  0.79	  0.77	  0.78	  0.79	  0.782
severe_toxic	  0.46	  0.42	  0.44	  0.43	  0.47	  0.444
obscene	       0.83	  0.82	  0.82	  0.81	  0.80	  0.816
threat	        0.40	  0.49	  0.40	  0.42	  0.42	  0.426
insult	        0.72	  0.72	  0.71	  0.71	  0.72  	0.716
identity_hate	 0.50	  0.49	  0.51	  0.49	  0.50	  0.498

CountVectorizer (max features, max_iter=5000)
Label	         Fold 1	Fold 2	Fold 3	Fold 4	Fold 5	Mean
toxic	         0.76	  0.77	  0.75	  0.76	  0.75	  0.758
severe_toxic	  0.42	  0.39	  0.42	  0.41	  0.43	  0.414
obscene	       0.80	  0.79  	0.80	  0.79	  0.80	  0.796
threat	        0.32	  0.35	  0.33	  0.26	  0.30	  0.312
insult	        0.69	  0.69	  0.69	  0.69	  0.68	  0.688
identity_hate	 0.44	  0.44	  0.46	  0.44	  0.45	  0.446




CNN Met Learning is TRUE
Learning rate 0.005
Trainable = True
Balanced uit
Classification report for label: toxic
              precision    recall  f1-score   support

           0       0.97      0.98      0.97     28834
           1       0.78      0.68      0.73      3059

    accuracy                           0.95     31893
   macro avg       0.87      0.83      0.85     31893
weighted avg       0.95      0.95      0.95     31893


Classification report for label: severe_toxic
              precision    recall  f1-score   support

           0       0.99      1.00      1.00     31608
           1       0.50      0.25      0.33       285

    accuracy                           0.99     31893
   macro avg       0.75      0.62      0.66     31893
weighted avg       0.99      0.99      0.99     31893


Classification report for label: obscene
              precision    recall  f1-score   support

           0       0.98      0.99      0.99     30199
           1       0.85      0.70      0.77      1694

    accuracy                           0.98     31893
   macro avg       0.92      0.85      0.88     31893
weighted avg       0.98      0.98      0.98     31893


Classification report for label: threat
              precision    recall  f1-score   support

           0       1.00      1.00      1.00     31798
           1       0.57      0.04      0.08        95

    accuracy                           1.00     31893
   macro avg       0.78      0.52      0.54     31893
weighted avg       1.00      1.00      1.00     31893


Classification report for label: insult
              precision    recall  f1-score   support

           0       0.98      0.99      0.98     30324
           1       0.71      0.60      0.65      1569

    accuracy                           0.97     31893
   macro avg       0.84      0.79      0.82     31893
weighted avg       0.97      0.97      0.97     31893


Classification report for label: identity_hate
              precision    recall  f1-score   support

           0       0.99      1.00      1.00     31632
           1       0.48      0.24      0.32       261

    accuracy                           0.99     31893
   macro avg       0.74      0.62      0.66     31893
weighted avg       0.99      0.99      0.99     31893


Overall Toxic vs Non-Toxic Classification:
              precision    recall  f1-score   support

           0       0.96      0.98      0.97     28650
           1       0.80      0.67      0.73      3243

    accuracy                           0.95     31893
   macro avg       0.88      0.83      0.85     31893
weighted avg       0.95      0.95      0.95     31893
