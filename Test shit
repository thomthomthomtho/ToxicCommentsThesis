import os
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, Concatenate, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, f1_score

# --- 1. Data inladen ---
current_directory = os.getcwd()
processed_path = os.path.join(current_directory, 'data', 'train_processed+.csv')
df = pd.read_csv(processed_path)
df = df[df['comment_text_clean'].notna()]

labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']

# --- 2. Tokenizen & Padding ---
MAX_NUM_WORDS = 20000  # max vocab size (beperkt tot top 20k woorden)
MAX_SEQUENCE_LENGTH = 200  # max lengte van elke comment (woorden)

tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)
tokenizer.fit_on_texts(df['comment_text_clean'])
sequences = tokenizer.texts_to_sequences(df['comment_text_clean'])

word_index = tokenizer.word_index
print(f"Found {len(word_index)} unique tokens.")

data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)

y = df[labels].values

# Train/test split, stratify op 'toxic' label om balans te houden
X_train, X_test, y_train, y_test = train_test_split(
    data, y, test_size=0.2, random_state=42, stratify=df['toxic']
)

# --- 3. GloVe embeddings inladen ---
glove_dir = os.path.join(current_directory, 'glove.6B')
glove_file = os.path.join(glove_dir, 'glove.6B.100d.txt')  # 100 dim embedding gekozen ivm paper

embeddings_index = {}
with open(glove_file, encoding='utf8') as f:
    for line in f:
        values = line.split()
        word = values[0]
        coefs = np.asarray(values[1:], dtype='float32')
        embeddings_index[word] = coefs
print(f"Found {len(embeddings_index)} word vectors in GloVe.")

embedding_dim = 100
num_words = min(MAX_NUM_WORDS, len(word_index) + 1)
embedding_matrix = np.zeros((num_words, embedding_dim))
for word, i in word_index.items():
    if i >= MAX_NUM_WORDS:
        continue
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        embedding_matrix[i] = embedding_vector
    # anders blijft het een vector van nullen (random init)

# --- 4. CNN-model bouwen gebaseerd op Kim (2014) + 2018 toxic comment paper ---
input_layer = Input(shape=(MAX_SEQUENCE_LENGTH,))

# Embeddinglaag met pre-trained GloVe embeddings, niet-trainbaar (volgens paper)
embedding_layer = Embedding(input_dim=num_words,
                            output_dim=embedding_dim,
                            weights=[embedding_matrix],
                            input_length=MAX_SEQUENCE_LENGTH,
                            trainable=False)(input_layer)

# Convolutionele filters met verschillende kernel sizes (3,4,5)
# Deze filters vangen verschillende n-gram patronen op in de tekst
conv_3 = Conv1D(filters=128, kernel_size=3, activation='relu')(embedding_layer)
conv_4 = Conv1D(filters=128, kernel_size=4, activation='relu')(embedding_layer)
conv_5 = Conv1D(filters=128, kernel_size=5, activation='relu')(embedding_layer)

# Max pooling: pakt belangrijkste features uit elke feature map
pool_3 = GlobalMaxPooling1D()(conv_3)
pool_4 = GlobalMaxPooling1D()(conv_4)
pool_5 = GlobalMaxPooling1D()(conv_5)

# Combineer de resultaten van de 3 filters tot 1 vector
concat = Concatenate()([pool_3, pool_4, pool_5])

# Dropout tegen overfitting (waarde uit de 2018 paper)
drop = Dropout(0.5)(concat)

# Outputlaag met 6 neuronen voor multi-label classificatie (sigmoid activatie)
output_layer = Dense(len(labels), activation='sigmoid')(drop)

model = Model(inputs=input_layer, outputs=output_layer)

# Compileer het model
# Learning rate = 0.001, Adam optimizer (zoals aanbevolen in 2018 paper)
model.compile(loss='binary_crossentropy',
              optimizer=Adam(learning_rate=0.001),
              metrics=['accuracy'])

model.summary()

# --- 5. Trainen ---
history = model.fit(X_train, y_train,
                    batch_size=64,      # batch size kleiner dan Kim (128) volgens 2018 paper
                    epochs=10,          # epochs iets hoger om stabiel te trainen
                    validation_split=0.1,
                    verbose=2)

# --- 6. Evalueren ---
y_pred_prob = model.predict(X_test)
y_pred = (y_pred_prob > 0.5).astype(int)

# Print classificatierapport per label (F1-score, precision, recall etc)
for i, label in enumerate(labels):
    print(f"\nClassification report for label: {label}")
    print(classification_report(y_test[:, i], y_pred[:, i]))

# Extra: overall toxic/not toxic classificatie (één of meer labels zijn 1)
y_test_overall = (y_test.sum(axis=1) > 0).astype(int)
y_pred_overall = (y_pred.sum(axis=1) > 0).astype(int)

print("\nOverall Toxic vs Non-Toxic Classification:")
print(classification_report(y_test_overall, y_pred_overall))
