import os
import numpy as np
import pandas as pd
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

# 1. Data inladen
df = pd.read_csv("data/train_processed+.csv")
df = df[df['comment_text_clean'].notna()]
labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']

# 2. Tokenizer instellen
MAX_NUM_WORDS = 2000000  # > 2 miljoen woorden, om geen vocab-limit te zetten zoals jij eerder deed
MAX_SEQUENCE_LENGTH = 100

tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)
tokenizer.fit_on_texts(df['comment_text_clean'])
sequences = tokenizer.texts_to_sequences(df['comment_text_clean'])
word_index = tokenizer.word_index
print(f"Found {len(word_index)} unique tokens.")

data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)
y = df[labels].values

# 3. Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    data, y, test_size=0.2, random_state=42, stratify=df['toxic']
)

# 4. GloVe Twitter-embeddings inladen
glove_path = os.path.join("glove.twitter.27B", "glove.twitter.27B.100d.txt")  # <-- Jouw GloVe!
embedding_dim = 100
embeddings_index = {}

with open(glove_path, encoding='utf-8') as f:
    for line in f:
        values = line.split()
        word = values[0]
        coefs = np.asarray(values[1:], dtype='float32')
        embeddings_index[word] = coefs
print(f"Loaded {len(embeddings_index)} word vectors from Twitter GloVe.")

# 5. Embedding matrix bouwen
num_words = min(MAX_NUM_WORDS, len(word_index) + 1)
embedding_matrix = np.zeros((num_words, embedding_dim))
for word, i in word_index.items():
    if i >= MAX_NUM_WORDS:
        continue
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        embedding_matrix[i] = embedding_vector

# 6. CNN model gebaseerd op Zhang et al. (2018)
model = Sequential()
model.add(Embedding(input_dim=num_words,
                    output_dim=embedding_dim,
                    weights=[embedding_matrix],
                    input_length=MAX_SEQUENCE_LENGTH,
                    trainable=False))  # embeddings frozen zoals in paper

model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))  # zoals paper beschrijft
model.add(GlobalMaxPooling1D())
model.add(Dropout(0.5))  # regularisatie
model.add(Dense(6, activation='sigmoid'))  # multi-label classificatie

model.compile(loss='binary_crossentropy',  # loss-functie past bij multilabel
              optimizer=Adam(learning_rate=1e-3),
              metrics=['accuracy'])

model.summary()

# 7. Model trainen
history = model.fit(X_train, y_train,
                    batch_size=128,
                    epochs=5,
                    validation_split=0.1)

# 8. Evaluatie
y_pred_prob = model.predict(X_test)
y_pred = (y_pred_prob > 0.5).astype(int)

for i, label in enumerate(labels):
    print(f"\nClassification report for label: {label}")
    print(classification_report(y_test[:, i], y_pred[:, i]))
